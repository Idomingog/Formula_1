{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping de los resultados históricos de la Formula 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es recopilar en un dataset los datos básicos de todas las carreras puntuables de la Formula 1.\n",
    "La Formula 1 es un deporte que se viene realizando formalmente desde 1050, donde las escuderías y los pilotos van mejorando continuamente. Cada año se realizan varias carreras en diferentes circuitos, habitualmente en diferentes países. \n",
    "Vamos a recopilar información de los resultados de la carrera puntuable para el campeonato, obteniendo la información básica sobre el circuito, el piloto, la escudería y el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raíz de la página sobre la que trabajaremos\n",
    "url_root = \"http://www.formula1.com\"\n",
    "# Página específica donde iniciaremos nuestra extracción de información\n",
    "url_start = \"https://www.formula1.com/en/results.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un dataset vacio con todas las columnas que vamos a obtener\n",
    "#df1 = pd.DataFrame(columns=['date'], dtype=str)\n",
    "#df2 = pd.DataFrame(columns=['country','circuit'], dtype=str)\n",
    "#df3 = pd.DataFrame(columns=['position','car_num'], dtype=int)\n",
    "#df4 = pd.DataFrame(columns=['name','surname','alias', 'team'], dtype=str)\n",
    "#df5 = pd.DataFrame(columns=['laps'], dtype=int)\n",
    "#df6 = pd.DataFrame(columns=['duration'], dtype=str)\n",
    "#df7 = pd.DataFrame(columns=['points'], dtype=int)\n",
    "#Formula_1 = pd.concat([df1, df2, df3, df4, df5, df6, df7], axis=1)\n",
    "Formula_1 = pd.DataFrame(columns=['date','country','circuit','position','car_num','name','surname','alias', 'team','laps','duration','points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar tres funciones diferentes, que son las que nos realizarán la conexión con la página web, verificarán si dicha conexión es correcta, y en caso afirmativo extraerán la información solicitada. \n",
    "La primera función extraerá la información de las url de cada año que ha habido en la Formula 1.\n",
    "La segunda función extraerá la información de las url de cada Gran premio de un año concreto.\n",
    "La tercera función extraerá la información de detalle de cada carrera carrera específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función extracción url relativos a los diferentes años de los que hay datos.\n",
    "\n",
    "def F1_year_extract(link):\n",
    "    # Realizamos la petición a la web de la Fórmula 1.\n",
    "    connection = requests.get(link)\n",
    "\n",
    "    # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "    statusCode = connection.status_code\n",
    "\n",
    "    # Si obtenemos los datos, realizamos el proceso, en caso contrario, terminamos.\n",
    "    if statusCode == 200:\n",
    "    \n",
    "        # Creamos una lista vacía para alojar las url de cada año\n",
    "        years_url_list = []\n",
    "    \n",
    "        # Descargamos la página raíz de los resultados\n",
    "        content = requests.get(link).text\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        \n",
    "        # Acotamos al código donde se especifican las url de cada año\n",
    "        cod_url_years = soup.find('div', {'class': 'resultsarchive-filter-container'})\n",
    "    \n",
    "        # Acotamos a cada división que contiene la url que buscamos\n",
    "        data_year = cod_url_years.find_all('li', {'class': 'resultsarchive-filter-item'})\n",
    "    \n",
    "        # Extraemos el año y la url donde buscaremos los resultados y lo incorporamos a la lista creada\n",
    "        for dy in data_year:\n",
    "            year = dy.find(\"span\").getText()\n",
    "            if year.isdigit():\n",
    "                link = dy.find(\"a\").get(\"href\")\n",
    "                url = \"%s%s\" % (url_root, link)\n",
    "                years_url_list.append(url)  # Lista \"path\" años.\n",
    "                \n",
    "                \n",
    "        return years_url_list\n",
    "    else:\n",
    "        print(\"Error de carga en la página inicial:\",link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función extracción url relativos a cada gran premio de cada año.\n",
    "\n",
    "def F1_prix_extract(link):\n",
    "    # Realizamos la petición a la web de la Fórmula 1.\n",
    "    connection = requests.get(link)\n",
    "\n",
    "    # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "    statusCode = connection.status_code\n",
    "\n",
    "    # Si obtenemos los datos, realizamos el proceso, en caso contrario, terminamos.\n",
    "    if statusCode == 200:\n",
    "        \n",
    "        # Creamos una lista vacía para almacenar los link a cada evento concreto\n",
    "        prix_url_list = []\n",
    "        \n",
    "        # Descargamos la página de cada año\n",
    "        content = requests.get(link).text\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        \n",
    "        \n",
    "        # Seleccionamos la división de donde extreremos los links de los eventos.\n",
    "        table = soup.find('table', {'class':'resultsarchive-table'})\n",
    "\n",
    "        table_body = table.find('tbody')\n",
    "        rows = table_body.find_all('tr')\n",
    "        for row in rows:\n",
    "            link = row.find('a', {'class':'dark bold ArchiveLink'}).get(\"href\")\n",
    "            country = row.find('a', {'href':link}).getText(strip=True)\n",
    "            date = row.find('td',{'class':'dark hide-for-mobile'}).getText(strip=True)\n",
    "            url = \"%s%s\" % (url_root, link)\n",
    "            prix_url_list.append((country,date,url))\n",
    "       \n",
    "        return(prix_url_list)\n",
    "      \n",
    "    else:\n",
    "        print(\"Error de carga en la página de un año concreto:\",link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función extracción información de resultados para cada gran premio.\n",
    "\n",
    "def F1_data_extract(link):\n",
    "        \n",
    "    # Realizamos la petición a la web de la Fórmula 1.\n",
    "    connection = requests.get(link)\n",
    "\n",
    "    # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "    statusCode = connection.status_code\n",
    "\n",
    "    # Si obtenemos los datos, realizamos el proceso, en caso contrario, terminamos.\n",
    "    if statusCode == 200:\n",
    "        \n",
    "        # Descargamos la página de cada evento\n",
    "        content = requests.get(link).text\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        \n",
    "        # Acotamos al código donde se especifica el nombre del circuito\n",
    "        circuit = soup.find('span', {'class': 'circuit-info'}).getText()\n",
    "        \n",
    "        table = soup.find('table', {'class':'resultsarchive-table'})\n",
    "        table_body = table.find('tbody')\n",
    "        rows = table_body.find_all('tr')\n",
    "        \n",
    "        records = []\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            pos = cols[1].getText()\n",
    "            no = cols[2].getText()\n",
    "            name = cols[3].find('span',{'class':'hide-for-tablet'}).getText()\n",
    "            surname = cols[3].find('span',{'class':'hide-for-mobile'}).getText()\n",
    "            alias = cols[3].find('span',{'class':'uppercase hide-for-desktop'}).getText()\n",
    "            team = cols[4].getText()\n",
    "            laps = cols[5].getText()\n",
    "            duration = cols[6].getText()\n",
    "            points = cols[7].getText()\n",
    "            records.append((circuit,pos,no,name,surname,alias,team,laps,duration,points))\n",
    "        \n",
    "        # Incorporamos un retraso de 1 segundo después de cara obtención de datos\n",
    "        time.sleep(1)\n",
    "        return(records)\n",
    "\n",
    "    else:\n",
    "        print(\"Error de carga en la página de un evento concreto:\",link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de este punto está el programa que irá utilizando las funciones creadas para la recolección de informacion, y la icorporará en el dataset. Finalmente escribimos el dataset obtenido en un fichero csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el listado de url de cada año.\n",
    "F1_url_by_year = F1_year_extract(url_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:54<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el listado de urls de cada gran premio según el año.\n",
    "F1_event_url_list = []\n",
    "\n",
    "# Vamos a extraer la información para cada url de cada año\n",
    "for season in tqdm(F1_url_by_year):\n",
    "    \n",
    "    # Extraemos la lista de url de eventos por cada año\n",
    "    url_event_list = F1_prix_extract(season)\n",
    "    \n",
    "    # Extraemos los datos de la lista para crear una única lista con la información que nos interesa.\n",
    "    for event in url_event_list:\n",
    "        country = event[0]\n",
    "        date = event[1]\n",
    "        link_e = event[2]\n",
    "        reg = (country,date,link_e)\n",
    "        # Esta lista contendrá todas las url de todos los eventos.\n",
    "        F1_event_url_list.append(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1033/1033 [37:36<00:00,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos los resultados de cada carrera\n",
    "\n",
    "#for race in F1_event_url_list:\n",
    "for race in tqdm(F1_event_url_list):\n",
    "    \n",
    "    # Identificamos la información que tenemos en cada elemento de la lista\n",
    "    country = race[0]\n",
    "    date = race[1]\n",
    "    link = race[2]\n",
    "    \n",
    "    # Extraemos los datos de cada carrera\n",
    "    data = F1_data_extract(link)\n",
    "    \n",
    "    #recorrer la lista data de los resultado de ese evento concreto\n",
    "    for result in data:\n",
    "        \n",
    "        date_f = datetime.strptime(date, '%d %b %Y').date()\n",
    "        \n",
    "        # Incorporar en el dataset cada registro.\n",
    "        new_reg = {'date':date_f,\n",
    "                   'country':country, \n",
    "                   'circuit':result[0],\n",
    "                   'position':result[1],\n",
    "                   'car_num':result[2],\n",
    "                   'name':result[3],\n",
    "                   'surname':result[4],\n",
    "                   'alias':result[5],\n",
    "                   'team':result[6],\n",
    "                   'laps':result[7],\n",
    "                   'duration':result[8],\n",
    "                   'points':result[9]\n",
    "                  }\n",
    "        Formula_1.loc[len(Formula_1)] = new_reg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribimos el dataset obtenido en un fichero csv\n",
    "Formula_1.to_csv('Formula_1_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
