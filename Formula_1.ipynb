{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping de los resultados históricos de la Formula 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es recopilar en un dataset los datos básicos de todas las carreras puntuables de la Formula 1.\n",
    "La Formula 1 es un deporte que se viene realizando formalmente desde 1050, donde las escuderías y los pilotos van mejorando continuamente. Cada año se realizan varias carreras en diferentes circuitos, habitualmente en diferentes países. \n",
    "Vamos a recopilar información de los resultados de la carrera puntuable para el campeonato, obteniendo la información básica sobre el circuito, el piloto, la escudería y el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raíz de la página sobre la que trabajaremos\n",
    "url_root = \"http://www.formula1.com\"\n",
    "# Página específica donde iniciaremos nuestra extracción de información\n",
    "url_start = \"https://www.formula1.com/en/results.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un dataset vacio con todas las columnas que vamos a obtener\n",
    "df1 = pd.DataFrame(columns=['date', 'country','circuit','position'], dtype=str)\n",
    "df2 = pd.DataFrame(columns=['car_num'], dtype=int)\n",
    "df3 = pd.DataFrame(columns=['name','surname','alias', 'team'], dtype=str)\n",
    "df4 = pd.DataFrame(columns=['laps'], dtype=int)\n",
    "df5 = pd.DataFrame(columns=['duration'], dtype=str)\n",
    "df6 = pd.DataFrame(columns=['points'], dtype=int)\n",
    "Formula_1 = pd.concat([df1, df2, df3, df4, df5, df6], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar tres funciones diferentes, que son las que nos realizarán la conexión con la página web, verificarán si dicha conexión es correcta, y en caso afirmativo extraerán la información solicitada. \n",
    "La primera función extraerá la información de las url de cada año que ha habido en la Formula 1.\n",
    "La segunda función extraerá la información de las url de cada Gran premio de un año concreto.\n",
    "La tercera función extraerá la información de detalle de cada carrera carrera específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función extracción url relativos a los diferentes años de los que hay datos.\n",
    "\n",
    "def F1_year_extract(link):\n",
    "\n",
    "    # Incorporamos un retraso de entre 0.5 segundos hasta 2 segundos después de cada obtención de datos\n",
    "    sleepTimes = [0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 2]\n",
    "    time.sleep(random.choice(sleepTimes))\n",
    "\n",
    "    # Realizamos la petición a la web de la Fórmula 1.\n",
    "    connection = requests.get(link)\n",
    "    \n",
    "    # Verificamos si la petición ha sido correcta, revisando el código que nos ha devuelto la petición.\n",
    "    if connection.status_code == 200:\n",
    "   \n",
    "        # Descargamos la página raíz de los resultados\n",
    "        soup = BeautifulSoup(connection.content, \"lxml\")\n",
    "        \n",
    "        # Creamos una lista vacía para alojar las url de cada año\n",
    "        years_url_list = []        \n",
    "        \n",
    "        # Acotamos al código donde se especifican las url de cada año\n",
    "        cod_url_years = soup.find('div', {'class': 'resultsarchive-filter-container'})\n",
    "    \n",
    "        # Buscamos cada atributo \"li\" que tiene el año definido como texto.\n",
    "        # Extraemos la url relativa, construimos la url completa y la guardamos.\n",
    "        for tag in cod_url_years.find_all(\"li\"):\n",
    "            if tag.text.strip('\\n').isdigit():\n",
    "                link = tag.find(\"a\").get(\"href\")\n",
    "                url = \"%s%s\" % (url_root, link)\n",
    "                years_url_list.append(url)  \n",
    "                \n",
    "        return years_url_list\n",
    "    else:\n",
    "        print(\"Error de carga en la página inicial:\",link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función extracción url relativos a cada gran premio de cada año.\n",
    "\n",
    "def F1_prix_extract(link):\n",
    "    \n",
    "    # Incorporamos un retraso de entre 0.5 segundos hasta 2 segundos después de cada obtención de datos\n",
    "    sleepTimes = [0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 2]\n",
    "    time.sleep(random.choice(sleepTimes))\n",
    "    \n",
    "    # Realizamos la petición a la url específica de un año concreto.\n",
    "    connection = requests.get(link)\n",
    "    \n",
    "    # Verificamos si la petición ha sido correcta, revisando el código que nos ha devuelto la petición.\n",
    "    if connection.status_code == 200:\n",
    "        \n",
    "        # Creamos una lista vacía para alojar la información de cada Gran Premio de Formula 1\n",
    "        prix_url_list = []\n",
    "        \n",
    "        # Descargamos la página de cada año\n",
    "        soup = BeautifulSoup(connection.content, \"lxml\")\n",
    "        \n",
    "        # Seleccionamos la división de donde extreremos los links de los eventos.\n",
    "        table = soup.find('table', {'class':'resultsarchive-table'})\n",
    "        \n",
    "        for event in table.tbody.find_all(\"tr\"):\n",
    "            country = event.a.text.strip('\\n ')\n",
    "            date = event.select('td')[2].text\n",
    "            link = event.a['href']\n",
    "            url = \"%s%s\" % (url_root, link)\n",
    "            prix_url_list.append((country,date,url))\n",
    "       \n",
    "        return(prix_url_list)\n",
    "      \n",
    "    else:\n",
    "        print(\"Error de carga en la página de un año concreto:\",link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función extracción información de resultados para cada gran premio.\n",
    "\n",
    "def F1_data_extract(link):\n",
    "\n",
    "    # Incorporamos un retraso de entre 0.5 segundos hasta 2 segundos después de cada obtención de datos\n",
    "    sleepTimes = [0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, 2]\n",
    "    time.sleep(random.choice(sleepTimes))\n",
    "    \n",
    "    # Realizamos la petición a la web de la Fórmula 1 y revisamos la codificación que tiene.\n",
    "    connection = requests.get(link)\n",
    "    http_encoding = connection.encoding if 'charset' in connection.headers.get('content-type', '').lower() else None\n",
    "    html_encoding = EncodingDetector.find_declared_encoding(connection.content, is_html=True)\n",
    "    encoding = html_encoding or http_encoding\n",
    "    \n",
    "    # Verificamos si la petición ha sido correcta, revisando el código que nos ha devuelto la petición.\n",
    "    if connection.status_code == 200:\n",
    "        \n",
    "        # Descargamos la página de cada evento\n",
    "        soup = BeautifulSoup(connection.content, \"lxml\", from_encoding=encoding)\n",
    "        \n",
    "        # Acotamos al código donde se especifica el nombre del circuito\n",
    "        circuit = soup.select('span.circuit-info')[0].text\n",
    "        \n",
    "        # Seleccionamos la tabla donde extraemos la información.\n",
    "        table = soup.find('table', {'class':'resultsarchive-table'})\n",
    "        \n",
    "        # Creamos una lista donde guardaremos la tupla de datos que obtengamos.\n",
    "        records = []\n",
    "        \n",
    "        # Recorremos la tabla fila a fila extrayendo la información que nos interesa\n",
    "        for event in table.tbody.find_all(\"tr\"):\n",
    "            pos = event.select('td')[1].text\n",
    "            no = event.select('td')[2].text\n",
    "            name = event.select('span')[0].text\n",
    "            surname = event.select('span')[1].text\n",
    "            alias = event.select('span')[2].text\n",
    "            team = event.select('td')[4].text\n",
    "            laps = event.select('td')[5].text\n",
    "            duration = event.select('td')[6].text\n",
    "            points = event.select('td')[7].text\n",
    "            records.append((circuit,pos,no,name,surname,alias,team,laps,duration,points))\n",
    "             \n",
    "\n",
    "        return(records)\n",
    "\n",
    "    else:\n",
    "        print(\"Error de carga en la página de un evento concreto:\",link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de este punto está el programa que irá utilizando las funciones creadas para la recolección de informacion, y la icorporará en el dataset. Finalmente escribimos el dataset obtenido en un fichero csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el listado de url de cada año.\n",
    "F1_url_by_year = F1_year_extract(url_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [01:53<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el listado de urls de cada gran premio según el año.\n",
    "F1_event_url_list = []\n",
    "\n",
    "# Vamos a extraer la información para cada url de cada año\n",
    "for season in tqdm(F1_url_by_year):\n",
    "    \n",
    "    # Extraemos la lista de url de eventos por cada año\n",
    "    url_event_list = F1_prix_extract(season)\n",
    "    \n",
    "    # Extraemos los datos de la lista para crear una única lista con la información que nos interesa.\n",
    "    for event in url_event_list:\n",
    "        country = event[0]\n",
    "        date = event[1]\n",
    "        link_e = event[2]\n",
    "        reg = (country,date,link_e)\n",
    "        \n",
    "        # Esta lista contendrá todas las url de todos los eventos.\n",
    "        F1_event_url_list.append(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1034/1034 [31:48<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos los resultados de cada carrera\n",
    "\n",
    "#for race in F1_event_url_list:\n",
    "for race in tqdm(F1_event_url_list):\n",
    "    \n",
    "    # Identificamos la información que tenemos en cada elemento de la lista\n",
    "    country = race[0]\n",
    "    date = race[1]\n",
    "    link = race[2]\n",
    "    \n",
    "    # Extraemos los datos de cada carrera\n",
    "    data = F1_data_extract(link)\n",
    "    \n",
    "    #recorrer la lista data de los resultado de ese evento concreto\n",
    "    for result in data:\n",
    "        \n",
    "        date_f = datetime.strptime(date, '%d %b %Y').date()\n",
    "        \n",
    "        # Incorporar en el dataset cada registro.\n",
    "        new_reg = {'date':date_f,\n",
    "                   'country':country, \n",
    "                   'circuit':result[0],\n",
    "                   'position':result[1],\n",
    "                   'car_num':int(result[2]),\n",
    "                   'name':result[3],\n",
    "                   'surname':result[4],\n",
    "                   'alias':result[5],\n",
    "                   'team':result[6],\n",
    "                   'laps':result[7],\n",
    "                   'duration':result[8],\n",
    "                   'points':float(result[9])\n",
    "                  }\n",
    "        Formula_1.loc[len(Formula_1)] = new_reg\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribimos el dataset obtenido en un fichero csv\n",
    "Formula_1.to_csv('Formula_1_historical_results.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
